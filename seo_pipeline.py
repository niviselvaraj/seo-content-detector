# -*- coding: utf-8 -*-
"""Seo pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fDtJ3za5E3thmPYJNBrJn_iL5wuSjY1G

1. Data Collection & HTML Parsing
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import csv

# Input and output file paths
input_file = "data.csv"        # your uploaded file containing URLs
output_file = "extracted_data.csv"

# Read URLs from CSV file (assume column name is 'url' or first column)
try:
    df = pd.read_csv(input_file)
    urls = df.iloc[:, 0].dropna().tolist()  # Take first column as URL list
except Exception as e:
    print(f"Error reading input file: {e}")
    urls = []

data = []

for url in urls:
    try:
        print(f"Processing: {url}")
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # Raise error for bad responses

        # Parse HTML content
        soup = BeautifulSoup(response.text, "html.parser")

        # Extract page title
        title = soup.title.string.strip() if soup.title else "No Title"

        # Extract and clean main body text
        for script in soup(["script", "style"]):
            script.decompose()
        text = soup.get_text(separator=' ', strip=True)
        word_count = len(text.split())

        data.append({
            "URL": url,
            "Title": title,
            "WordCount": word_count,
            "Text": text
        })

    except Exception as e:
        print(f"Error processing {url}: {e}")
        data.append({
            "URL": url,
            "Title": "Error",
            "WordCount": 0,
            "Text": ""
        })

# Save results to CSV
try:
    pd.DataFrame(data).to_csv(output_file, index=False, quoting=csv.QUOTE_ALL)
    print(f"Extraction complete! Saved to {output_file}")
except Exception as e:
    print(f"Error saving file: {e}")

clean_data=pd.read_csv("extracted_data.csv")
print(clean_data)

"""2. Text Preprocessing & Feature Engineering"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer
import numpy as np
import csv

# Download NLTK data (run once)
nltk.download('punkt')
nltk.download('punkt_tab')

# Input/Output
input_file = "data.csv"
output_file = "extracted_features.csv"

# Load URLs
try:
    df = pd.read_csv(input_file)
    urls = df.iloc[:, 0].dropna().tolist()
except Exception as e:
    print(f"Error reading {input_file}: {e}")
    urls = []

# Load embedding model (you can change this to a smaller one if needed)
model = SentenceTransformer("all-MiniLM-L6-v2")

def clean_text(text):
    text = text.lower()
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_text_from_url(url):
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        text = soup.get_text(separator=' ')
        return clean_text(text)
    except Exception as e:
        print(f"Error extracting from {url}: {e}")
        return ""

def get_top_keywords(text, top_n=5):
    try:
        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        tfidf_matrix = vectorizer.fit_transform([text])
        scores = tfidf_matrix.toarray()[0]
        words = vectorizer.get_feature_names_out()
        top_indices = scores.argsort()[-top_n:][::-1]
        top_words = [words[i] for i in top_indices]
        return "|".join(top_words)
    except:
        return ""

data = []

for url in urls:
    print(f"Processing: {url}")
    text = extract_text_from_url(url)
    if not text:
        data.append({
            "url": url,
            "word_count": 0,
            "sentence_count": 0,

            "top_keywords": "",
            "embedding": "[]"
        })
        continue

    # Basic metrics
    word_count = len(word_tokenize(text))
    sentence_count = len(sent_tokenize(text))


    # Top keywords
    top_keywords = get_top_keywords(text)

    # Embedding vector
    embedding = model.encode(text[:5000])  # limit length for safety
    embedding_str = "[" + ",".join([f"{x:.3f}" for x in embedding.tolist()]) + "]"

    data.append({
        "url": url,
        "word_count": word_count,
        "sentence_count": sentence_count,

        "top_keywords": top_keywords,
        "embedding": embedding_str
    })

# Save results
pd.DataFrame(data).to_csv(output_file, index=False, quoting=csv.QUOTE_ALL)
print(f"\n‚úÖ Extraction complete! Saved to {output_file}")

data=pd.read_csv("extracted_features.csv")
print(data)

"""3. Duplicate Detection"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import csv
import ast

# =========================================================
# CONFIGURATION
# =========================================================
input_file = "extracted_features.csv"
output_duplicates = "duplicate_pairs.csv"
output_summary = "content_summary.csv"

similarity_threshold = 0.80
thin_word_limit = 500
expected_dim = 384   # default for all-MiniLM-L6-v2 embeddings
# =========================================================

# ---------------------------------------------------------
# STEP 1: Load CSV
# ---------------------------------------------------------
try:
    df = pd.read_csv(input_file)
except Exception as e:
    raise RuntimeError(f"‚ùå Error reading {input_file}: {e}")

if "embedding" not in df.columns:
    raise ValueError("‚ùå 'embedding' column not found in input file!")

# ---------------------------------------------------------
# STEP 2: Parse and fix embeddings
# ---------------------------------------------------------
def parse_embedding_safe(emb_str):
    try:
        if isinstance(emb_str, str) and emb_str.strip() not in ["[]", "", "nan"]:
            emb = np.array(ast.literal_eval(emb_str), dtype=float)
            # Handle wrong dimension
            if emb.size == 0:
                return np.zeros(expected_dim)
            return emb
        else:
            return np.zeros(expected_dim)
    except:
        return np.zeros(expected_dim)

df["embedding"] = df["embedding"].apply(parse_embedding_safe)

# ---------------------------------------------------------
# STEP 3: Thin Content Flag
# ---------------------------------------------------------
df["is_thin"] = (df["word_count"] < thin_word_limit).astype(int)

# ---------------------------------------------------------
# STEP 4: Compute cosine similarity safely
# ---------------------------------------------------------
print("üìà Computing cosine similarity on embeddings...")

embeddings = np.vstack(df["embedding"].to_numpy())
similarity_matrix = cosine_similarity(embeddings)

# ---------------------------------------------------------
# STEP 5: Identify duplicates above threshold
# ---------------------------------------------------------
duplicate_pairs = []
n = len(df)

for i in range(n):
    for j in range(i + 1, n):
        sim = similarity_matrix[i, j]
        if sim > similarity_threshold:
            duplicate_pairs.append({
                "url1": df.iloc[i]["url"],
                "url2": df.iloc[j]["url"],
                "similarity": round(sim, 3)
            })

duplicates_df = pd.DataFrame(duplicate_pairs)
duplicates_df.to_csv(output_duplicates, index=False, quoting=csv.QUOTE_ALL)

# ---------------------------------------------------------
# STEP 6: Summary
# ---------------------------------------------------------
total_pages = len(df)
duplicate_count = len(duplicates_df)
thin_pages = df["is_thin"].sum()
thin_percent = round((thin_pages / total_pages) * 100, 2)

summary = {
    "Total pages analyzed": total_pages,
    "Duplicate pairs": duplicate_count,
    "Thin content pages": thin_pages,
    "Thin content percentage": f"{thin_percent}%"
}

summary_df = pd.DataFrame([summary])
summary_df.to_csv(output_summary, index=False, quoting=csv.QUOTE_ALL)

# ---------------------------------------------------------
# OUTPUT
# ---------------------------------------------------------
print("\n‚úÖ Analysis complete!")
print(f"Total pages analyzed: {total_pages}")
print(f"Duplicate pairs found: {duplicate_count}")
print(f"Thin content pages: {thin_pages} ({thin_percent}%)")
print(f"\nResults saved to:\n - {output_duplicates}\n - {output_summary}")

duplicate=pd.read_csv("duplicate_pairs.csv")
print(duplicate)
summary=pd.read_csv("content_summary.csv")
print(summary)

"""4. Content Quality Scoring"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# =========================================================
# CONFIGURATION
# =========================================================
input_file = "extracted_features.csv"
target_column = "quality_label"

# =========================================================
# STEP 1: Load data
# =========================================================
df = pd.read_csv(input_file)

# If Flesch score missing, simulate based on sentence-to-word ratio
if "flesch_reading_ease" not in df.columns:
    print("‚ÑπÔ∏è Estimating Flesch Reading Ease using proxy formula...")
    # approximate readability = 206.835 - 1.015*(avg sentence length) - 84.6*(avg word length)
    # since we lack word length info, we‚Äôll use sentence_count & word_count ratio
    df["avg_sentence_length"] = df["word_count"] / (df["sentence_count"] + 1)
    df["flesch_reading_ease"] = 206.835 - 1.015 * df["avg_sentence_length"]
    df["flesch_reading_ease"] = df["flesch_reading_ease"].clip(lower=0, upper=100)

# Check for required columns
required_cols = ["url", "word_count", "sentence_count", "flesch_reading_ease"]
for col in required_cols:
    if col not in df.columns:
        raise ValueError(f"‚ùå Missing required column: {col}")

# =========================================================
# STEP 2: Create synthetic quality labels
# =========================================================
def assign_quality(row):
    wc = row["word_count"]
    fr = row["flesch_reading_ease"]

    if wc > 1500 and 50 <= fr <= 70:
        return "High"
    elif wc < 500 or fr < 30:
        return "Low"
    else:
        return "Medium"

df[target_column] = df.apply(assign_quality, axis=1)

# =========================================================
# STEP 3: Train/Test Split
# =========================================================
X = df[["word_count", "sentence_count", "flesch_reading_ease"]]
y = df[target_column]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# =========================================================
# STEP 4: Model Training
# =========================================================
model = RandomForestClassifier(random_state=42, n_estimators=100)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# =========================================================
# STEP 5: Evaluation
# =========================================================
print("\n=== Model Performance ===")
print(classification_report(y_test, y_pred))
print(f"Overall Accuracy: {accuracy_score(y_test, y_pred):.2f}")

cm = confusion_matrix(y_test, y_pred, labels=["Low", "Medium", "High"])
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Low", "Medium", "High"],
            yticklabels=["Low", "Medium", "High"])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# =========================================================
# STEP 6: Baseline (Rule-based)
# =========================================================
def baseline_classifier(wc):
    if wc > 1500:
        return "High"
    elif wc < 500:
        return "Low"
    else:
        return "Medium"

baseline_pred = X_test["word_count"].apply(baseline_classifier)
baseline_acc = accuracy_score(y_test, baseline_pred)
print(f"\nBaseline Accuracy: {baseline_acc:.2f}")

# =========================================================
# STEP 7: Feature Importance
# =========================================================
importances = pd.Series(model.feature_importances_, index=X.columns)
top_features = importances.sort_values(ascending=False)

print("\nTop Features:")
for feature, imp in top_features.items():
    print(f"- {feature}: {imp:.2f}")

sns.barplot(x=importances.values, y=importances.index, palette="coolwarm")
plt.title("Feature Importance")
plt.show()

# =========================================================
# STEP 8: Save results
# =========================================================
results = pd.DataFrame({
    "url": df.loc[y_test.index, "url"],
    "actual_label": y_test,
    "predicted_label": y_pred
})
results.to_csv("content_quality_predictions.csv", index=False)
print("\n‚úÖ Results saved to content_quality_predictions.csv")

data=pd.read_csv("content_quality_predictions.csv")
print(data)

"""5. Real-Time Analysis Demo"""

import requests
from bs4 import BeautifulSoup
import re
import numpy as np
import pandas as pd
import json
import torch
from sentence_transformers import SentenceTransformer, util

# Load existing dataset
existing_df = pd.read_csv("extracted_features.csv")

# Convert embeddings safely to float32 arrays
def parse_embedding(x):
    try:
        arr = np.array(json.loads(x), dtype=np.float32)
        return arr if arr.size > 0 else np.zeros(384, dtype=np.float32)
    except:
        return np.zeros(384, dtype=np.float32)

existing_df["embedding"] = existing_df["embedding"].apply(parse_embedding)

# Load transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

def clean_text(text):
    text = re.sub(r"\s+", " ", text)
    return text.strip().lower()

def flesch_reading_ease(words, sentences):
    if sentences == 0:
        return 0
    avg_sentence_length = words / sentences
    # simplified Flesch approximation
    return round(206.835 - 1.015 * avg_sentence_length, 2)

def classify_quality(word_count, readability):
    if word_count > 1500 and 50 <= readability <= 70:
        return "High"
    elif word_count < 500 or readability < 30:
        return "Low"
    else:
        return "Medium"

def analyze_url(url, similarity_threshold=0.75):
    try:
        # Step 1: Scrape page
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")

        # Step 2: Extract text
        title = soup.title.string if soup.title else ""
        paragraphs = [p.get_text() for p in soup.find_all("p")]
        text = clean_text(" ".join(paragraphs))
        if not text:
            return {"error": "No readable content found"}

        # Step 3: Extract features
        words = len(text.split())
        sentences = len(re.findall(r"[.!?]", text))
        readability = flesch_reading_ease(words, sentences)
        quality_label = classify_quality(words, readability)
        is_thin = words < 500

        # Step 4: Compute embedding (float32)
        new_embedding = model.encode(text, convert_to_tensor=True).to("cpu").to(torch.float32)

        # Step 5: Compute similarities
        similarities = []
        for _, row in existing_df.iterrows():
            existing_emb = torch.tensor(row["embedding"], dtype=torch.float32)
            sim = float(util.cos_sim(new_embedding, existing_emb))
            if sim > similarity_threshold:
                similarities.append({"url": row["url"], "similarity": round(sim, 2)})

        # Step 6: Return structured result
        result = {
            "url": url,
            "title": title,
            "word_count": words,
            "readability": readability,
            "quality_label": quality_label,
            "is_thin": is_thin,
            "similar_to": similarities
        }
        return result

    except Exception as e:
        return {"url": url, "error": str(e)}

result = analyze_url("https://www.varonis.com/blog/cybersecurity-tips")
print(json.dumps(result, indent=2))

result = analyze_url("https://www.geeksforgeeks.org/machine-learning/supervised-machine-learning/")
print(json.dumps(result, indent=2))